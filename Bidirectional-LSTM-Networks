#This script builds and trains a Bidirectional Long Short-Term Memory (Bi-LSTM) model to perform sentiment analysis on movie reviews from the IMDB dataset. 
#The goal is to classify reviews as either positive (1) or negative (0).

import numpy as np  # For numerical operations and handling arrays
import tensorflow as tf  # TensorFlow for deep learning and neural networks
from keras.models import Sequential  # Sequential model API for building neural networks
from keras.preprocessing import sequence  # For preprocessing text data (padding sequences)
from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional  # Layers for deep learning models
from matplotlib import pyplot  # For data visualization and plotting results

# Load IMDB dataset with a vocabulary size limit of 10,000 unique words
VOCAB_SIZE = 10000
(TRAIN_REVIEWS, TRAIN_LABELS), (TEST_REVIEWS, TEST_LABELS) = tf.keras.datasets.imdb.load_data(
    path='imdb.npz', num_words=VOCAB_SIZE, skip_top=0, maxlen=None, 
    seed=113, start_char=1, oov_char=2, index_from=3
)

# Pad sequences to ensure all reviews have the same length (200 words)
MAX_SEQUENCE_LENGTH = 200
TRAIN_REVIEWS = sequence.pad_sequences(TRAIN_REVIEWS, maxlen=MAX_SEQUENCE_LENGTH)
TEST_REVIEWS = sequence.pad_sequences(TEST_REVIEWS, maxlen=MAX_SEQUENCE_LENGTH)
TRAIN_LABELS = np.array(TRAIN_LABELS)
TEST_LABELS = np.array(TEST_LABELS)

# Define a Bidirectional LSTM model for sentiment analysis
bi_lstm_model = Sequential()
bi_lstm_model.add(Embedding(VOCAB_SIZE, 128, input_length=MAX_SEQUENCE_LENGTH))  # Embedding layer converts words to dense vectors
bi_lstm_model.add(Bidirectional(LSTM(64)))  # Bi-LSTM layer captures context from both forward and backward directions
bi_lstm_model.add(Dropout(0.5))  # Dropout layer prevents overfitting by randomly deactivating neurons during training
bi_lstm_model.add(Dense(1, activation='sigmoid'))  # Fully connected layer with sigmoid activation for binary classification
bi_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Compile model with binary cross-entropy loss

# Train the model with a batch size of 32 and 12 epochs
BATCH_SIZE = 32
training_history = bi_lstm_model.fit(TRAIN_REVIEWS, TRAIN_LABELS, batch_size=BATCH_SIZE, epochs=12, validation_data=(TEST_REVIEWS, TEST_LABELS))

# Print training history for loss and accuracy
print("Training Loss:", training_history.history['loss'])
print("Training Accuracy:", training_history.history['accuracy'])

# Plot training loss and accuracy over epochs
pyplot.plot(training_history.history['loss'], label='Loss')
pyplot.plot(training_history.history['accuracy'], label='Accuracy')
pyplot.title('Model Loss vs Accuracy')
pyplot.xlabel('Epoch')
pyplot.ylabel('Value')
pyplot.legend(loc='upper right')
pyplot.show()
